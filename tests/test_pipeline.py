"""
Integration tests for the full pipeline.

These tests run the pipeline against the real (or a temporary) dataset
to verify end-to-end correctness of all output files.
"""

import json
import sys
import tempfile
from pathlib import Path

import pandas as pd
import pytest

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from src import config


# ---------------------------------------------------------------------------
# Fixtures
# ---------------------------------------------------------------------------

@pytest.fixture(scope="module")
def pipeline_outputs():
    """
    Verify that pipeline outputs exist (generated by running `python -m src.pipeline`).
    If not, skip these tests gracefully.
    """
    required = [
        config.SCORED_REVIEWS_PARQUET,
        config.FEATURE_SUMMARY_CSV,
        config.KEYWORD_SUMMARY_CSV,
        config.TIME_TRENDS_CSV,
        config.METRICS_JSON,
    ]
    missing = [str(p) for p in required if not Path(p).exists()]
    if missing:
        pytest.skip(f"Pipeline outputs not found: {missing}. Run `python -m src.pipeline` first.")

    return {
        "scored": pd.read_parquet(config.SCORED_REVIEWS_PARQUET),
        "features": pd.read_csv(config.FEATURE_SUMMARY_CSV),
        "keywords": pd.read_csv(config.KEYWORD_SUMMARY_CSV),
        "trends": pd.read_csv(config.TIME_TRENDS_CSV),
        "metrics": json.loads(config.METRICS_JSON.read_text()),
    }


# ---------------------------------------------------------------------------
# Scored reviews
# ---------------------------------------------------------------------------

class TestScoredReviews:
    def test_min_row_count(self, pipeline_outputs):
        df = pipeline_outputs["scored"]
        assert len(df) >= 9_000, f"Expected â‰¥9000 rows, got {len(df)}"

    def test_required_columns(self, pipeline_outputs):
        df = pipeline_outputs["scored"]
        required = {
            "review_id", "brand", "rating", "review_text", "review_date",
            "verified_purchase", "country", "price_paid", "return_flag",
            "features_mentioned", "ground_truth_sentiment",
            "clean_vader", "clean_tfidf",
            "sentiment_score", "predicted_sentiment",
        }
        missing = required - set(df.columns)
        assert not missing, f"Missing columns: {missing}"

    def test_sentiment_labels_valid(self, pipeline_outputs):
        df = pipeline_outputs["scored"]
        valid = {"positive", "negative", "neutral"}
        assert set(df["predicted_sentiment"].unique()).issubset(valid)

    def test_sentiment_scores_in_range(self, pipeline_outputs):
        df = pipeline_outputs["scored"]
        assert df["sentiment_score"].between(-1.0, 1.0).all()

    def test_no_null_predictions(self, pipeline_outputs):
        df = pipeline_outputs["scored"]
        assert df["predicted_sentiment"].isna().sum() == 0


# ---------------------------------------------------------------------------
# Feature summary
# ---------------------------------------------------------------------------

class TestFeatureSummary:
    def test_has_features(self, pipeline_outputs):
        feat = pipeline_outputs["features"]
        assert len(feat) >= 10, "Expected at least 10 features"

    def test_required_columns(self, pipeline_outputs):
        feat = pipeline_outputs["features"]
        required = {"feature", "total_mentions", "negative_mentions", "negative_rate", "positive_rate"}
        assert required.issubset(set(feat.columns))

    def test_negative_rate_in_range(self, pipeline_outputs):
        feat = pipeline_outputs["features"]
        assert feat["negative_rate"].between(0, 1).all()

    def test_positive_rate_in_range(self, pipeline_outputs):
        feat = pipeline_outputs["features"]
        assert feat["positive_rate"].between(0, 1).all()

    def test_total_mentions_positive(self, pipeline_outputs):
        feat = pipeline_outputs["features"]
        assert (feat["total_mentions"] > 0).all()


# ---------------------------------------------------------------------------
# Keyword summary
# ---------------------------------------------------------------------------

class TestKeywordSummary:
    def test_sentiments_covered(self, pipeline_outputs):
        kw = pipeline_outputs["keywords"]
        sentiments = set(kw["sentiment"].unique())
        assert {"positive", "negative", "overall"}.issubset(sentiments)

    def test_keywords_non_empty(self, pipeline_outputs):
        kw = pipeline_outputs["keywords"]
        assert (kw["keyword"].str.len() > 0).all()

    def test_tfidf_scores_positive(self, pipeline_outputs):
        kw = pipeline_outputs["keywords"]
        assert (kw["tfidf_score"] >= 0).all()


# ---------------------------------------------------------------------------
# Metrics
# ---------------------------------------------------------------------------

class TestMetrics:
    def test_accuracy_reasonable(self, pipeline_outputs):
        metrics = pipeline_outputs["metrics"]
        acc = metrics.get("accuracy", 0)
        assert 0.50 <= acc <= 1.0, f"Suspiciously low accuracy: {acc}"

    def test_kpis_present(self, pipeline_outputs):
        kpis = pipeline_outputs["metrics"].get("kpis", {})
        assert "total_reviews" in kpis
        assert "positive_pct" in kpis
        assert "negative_pct" in kpis
        assert "mean_rating" in kpis

    def test_recommendations_generated(self, pipeline_outputs):
        recs = pipeline_outputs["metrics"].get("recommendations", [])
        assert len(recs) >= 1

    def test_recommendation_structure(self, pipeline_outputs):
        recs = pipeline_outputs["metrics"].get("recommendations", [])
        for rec in recs:
            assert "category" in rec
            assert "insight" in rec
            assert "severity" in rec
            assert rec["severity"] in {"high", "medium", "low"}
